---
title: "Air quality report - data exploration"
format: 
    html:
        eval: false
        code-fold: true
        code-overflow: scroll
---

## Introduction

This is an exploratory analysis of air quality reports for Catalonia region, updated as of 14/05/2025. 
Data is downloaded from the [open data portal](https://administraciodigital.gencat.cat/ca/dades/dades-obertes/inici/index.html#googtrans(ca|en)) 
of Catalan Government (Generalitat de Catalunya), and includes hourly air quality metrics as well as 
geospatial information of the capture points.

The objective of this analysis is to find the biggest contributors to air pollution in Catalonia for the time period of the data. In order to get to this, both time evolution and geographic position will be taken into account.

## Project workflow

The analysis is structured around a [kedro](https://docs.kedro.org/en/stable/introduction/index.html) 
basic workflow, written in Python and uses the following tools / packages:

- uv (package dependency manager)
- quarto (report renderer)
- pandas (data wrangling)
- altair (data visualization)
- geopandas (geospatial analysis)

The diagram of the different processes applied to the data is shown below:

- Data ingestion and type checking: reading the raw CSV file, converting to appropriate data types, saving "bronze" data to a parquet file.
- Missing data handling: analyzing missing data and handling it properly (removing or imputing), saving "bronze" data to a parquet file.

WIP --> to be updated as the analysis progresses.

```{python}
# Print basic information about the environment and tools used
py_ver = !python --version
print(f"Python version (virtual environment):{py_ver[0]}\n\n")

!kedro -V
```

## Data structure

The raw data is a unique CSV file with the following columns and data types:

```{python}
# Load kedro catalog from YAML
from kedro.io import KedroDataCatalog
import pandas as pd
import yaml

with open("../conf/base/catalog.yml", "r") as f:
    conf_catalog = yaml.safe_load(f)

catalog = KedroDataCatalog.from_config(conf_catalog)
```

```{python}
# Load raw data from the catalog
df_raw = catalog.load("air_quality_log")
df_raw.info()
```

The contained data can be divided in the following categories:

- Identification: columns 0, 1
- Time: column 2 (and partially columns 12-35)
- Location: columns 6, 7, 8, 9, 10, 11, 36, 37, 38, 39 (empty)
- Measurement info: columns 3, 4, 5, 12-35

Data structure doesn't follow a strict "row per measurement" lemma, 
as every row of the file contains 24 data measurements (hourly average per each day).

Some rework will have to be done for easier anaylisis, pivoting the data in order to have
one row for each time measurement per station and per parameter.

Data types seem apropiate to the content. Once pivoted, however, new data types will have to be added, 
such as timestamp for the combined date + hour information. The geographic information, which is plain text
or floats, will have to be converted for geospatial analysis. This will be done by converting the whole data 
type from a pandas normal DataFrame to a geopandas GeoDataFrame (which will retain all of its previous columns plus a 'geometry' column).

However, as the raw data contains 1.4M+ rows (and we will be expanding it even more), the first step will be to split the big dataframe into smaller chunks. Then each chunk will be processed separately and saved as a parquet file.

```{python}
# Pivot the data and save as parquet

def pivoting_raw_data(df_raw: pd.DataFrame) -> pd.DataFrame:
    df_pivoted = (
        df_raw
        .drop(columns=["Georefer√®ncia"])
        .melt(id_vars=["CODI EOI", "NOM ESTACIO", "DATA", "MAGNITUD", "CONTAMINANT", "UNITATS", "TIPUS ESTACIO", "AREA URBANA", 
        "CODI INE", "MUNICIPI", "CODI COMARCA", "NOM COMARCA", "ALTITUD", "LATITUD", "LONGITUD"],
        value_vars=["01h", "02h", "03h", "04h", "05h", "06h", "07h", "08h", "09h", "10h", "11h", "12h", "13h",
        "14h", "15h", "16h", "17h", "18h", "19h", "20h", "21h", "22h", "23h", "24h"],
        var_name="HORA", value_name="VALOR")
    )
    return df_pivoted

print(f"Total number of rows: {df_raw.shape[0]}")
max_rows_per_chunk = 350000

index = 1
for i in range(0, df_raw.shape[0], max_rows_per_chunk):
    chunk = df_raw.iloc[i:i+max_rows_per_chunk,:]
    pivoted_chunk = pivoting_raw_data(chunk)
    #df_bronze = pd.concat([df_bronze, pivoted_chunk], ignore_index=True)
    pivoted_chunk.to_parquet(f"../data/02_intermediate/air_quality_bronze_{index}.parquet", engine="pyarrow", partition_cols=["NOM COMARCA"])
    index += 1

del df_raw  
```

For simplicity, let's work with only one of the chunks. We will use the first chunk for this purpose (newest data).
```{python}
# Load the first chunk of data.
df_bronze = pd.read_parquet("../data/02_intermediate/air_quality_bronze_1.parquet")
```

```{python}
# Convert the date and time columns to a single timestamp column.
from datetime import datetime, timedelta


df_bronze.loc[:, "HORA"] = df_bronze.loc[:, "HORA"].str.replace("h", "").astype(int)-1

df_bronze = df_bronze.assign(
    DATA_HORA=lambda s: s.apply(
        lambda row: datetime.strptime(
            row.loc["DATA"] + " " + str(row.loc["HORA"]) + ":00"
        , "%d/%m/%Y %H:%M"
        ) + timedelta(hours=1),
    axis=1
)
).drop(columns=["DATA", "HORA"])
```


```{python}
# Create a geodataframe with the geometry taken from the coordinates in "LATITUD" and "LONGITUD"
import geopandas as gpd

gdf_bronze = gpd.GeoDataFrame(
    df_bronze, geometry=gpd.points_from_xy(
        df_bronze.loc[:,"LATITUD"], df_bronze.loc[:,"LONGITUD"],crs="EPSG:4326"
    )
).drop(columns=["LATITUD", "LONGITUD"])

del df_bronze
gdf_bronze.head()
```

## Peek of the data

Let's select a close-by station and plot a few air quality parameters to see how they look like.

```{python}
# Select a station and some air quality magnitudes
import altair as alt

station = "Barcelona (Eixample)"
magnitudes = gdf_bronze.loc[gdf_bronze.loc[:,"NOM ESTACIO"] == station,"CONTAMINANT"].unique()

closeby_station = gdf_bronze.query("`NOM ESTACIO` == @station").loc[:,["DATA_HORA","VALOR", "CONTAMINANT", "UNITATS"]].sort_values(by="DATA_HORA")

unit = random_parameter.loc[:,"UNITATS"].unique()[0]

print(f"Chosen station: {station}")
print(f"Available magnitudes: {magnitudes}")
```

```{python}
# Plot the data of the latest 200 measurements for a random parameter
magnitude = closeby_station.loc[:,"CONTAMINANT"].sample(1).values[0]
unit = closeby_station.query("`CONTAMINANT` == @magnitude").loc[:,"UNITATS"].values[0]

alt.Chart(closeby_station.query("`CONTAMINANT` == @magnitude").tail(200)).mark_line().encode(
    x=alt.X("DATA_HORA:T", title="Date"),
    y=alt.Y("VALOR:Q", title=f"Air Quality Parameter {magnitude} [{unit}]"),
    tooltip=["DATA_HORA", "VALOR"]
)
```

