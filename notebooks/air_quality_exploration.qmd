---
title: "Air quality report - data exploration"
format: 
    html:
        # eval: false
        code-fold: true
        code-overflow: scroll
---

## Introduction

This is an exploratory analysis of air quality reports for Catalonia region, updated as of 14/05/2025. 
Data is downloaded from the [open data portal](https://administraciodigital.gencat.cat/ca/dades/dades-obertes/inici/index.html#googtrans(ca|en)) 
of Catalan Government (Generalitat de Catalunya), and includes hourly air quality metrics as well as 
geospatial information of the capture points.

The objective of this analysis is to find the biggest contributors to air pollution in Catalonia for the time period of the data. In order to get to this, both time evolution and geographic position will be taken into account.

## Project workflow

The analysis will be written in Python and uses the following tools / packages:

- uv (package dependency manager)
- quarto (report renderer)
- pandas (data wrangling) via dask
- geopandas (geospatial analysis) via dask-geopandas
- altair (data visualization)
- darts (time series analysis)

The diagram of the different processes applied to the data is shown below:

- [x] Data ingestion and type checking: reading the raw CSV file, converting to appropriate data types, saving data to a parquet file.
- [x] Missing data handling: analyzing missing data and handling it properly (removing or imputing), saving "bronze" data to a parquet file.
- [ ] General exploration: checking data overview to spot inconsistencies and get a general grasp of the data "shape".
- [ ] Feature creation: adding new features or characteristics based on geographic knowledge or specific thematic knowledge (air pollution).
- [ ] Time evolution: checking trends through time in the general dataset.
- [ ] Geographic inequalities: checking differences between geographic points and their relationship to them.
- [ ] Combined analysis: mixing time evolution and geographic inequialities to identify broader patterns.

**WIP --> to be updated as the analysis progresses.**

```{python}
#| eval: false

# Print basic information about the environment and tools used
py_ver = !python --version
print(f"Python version (virtual environment):{py_ver[0]}")
```

## Data structure

The raw data is a unique CSV file with the following columns and data types:

```{python}
#| output: false

# Import necessary modules
import random as random
import os
import glob
from pathlib import Path
from datetime import datetime, timedelta

import pandas as pd
import dask.dataframe as dd
import numpy as np
import geopandas as gpd
import dask_geopandas
import missingno as msno
import altair as alt
import folium

cwd = Path.cwd()
data_raw = cwd.parent.joinpath("data/01_raw")
data_intermediate = cwd.parent.joinpath("data/02_intermediate")
data_primary = cwd.parent.joinpath("data/03_primary")
raw_file = data_raw.join
```

```{python}
# Load raw data from the catalog
df_raw = pd
dd_raw = dd.from_pandas(df_raw, chunksize=context.params["max_raw_rows"])
df_raw.info()
del df_raw
```

```{python}
#| eval: false

# Use only first partition(s) to optimize EDA
dd_raw = dd_raw.partitions[0:2]
```

The contained data can be divided in the following categories:

- Identification: columns 0, 1
- Time: column 2 (and partially columns 12-35)
- Location: columns 6, 7, 8, 9, 10, 11, 36, 37, 38, 39 (empty)
- Measurement info: columns 3, 4, 5, 12-35

Data structure doesn't follow a strict "row per measurement" lemma, 
as every row of the file contains 24 data measurements (hourly average per each day).

Some rework will have to be done for easier anaylisis, pivoting the data in order to have
one row for each time measurement per station and per parameter.

Data types seem apropiate to the content. Once pivoted, however, new data types will have to be added, 
such as timestamp for the combined date + hour information. The geographic information, which is plain text
or floats, will have to be converted for geospatial analysis. This will be done by converting the whole data 
type from a pandas normal DataFrame to a geopandas GeoDataFrame (which will retain all of its previous columns plus a 'geometry' column).

```{python}
#| eval: false

# Pivot the data
def pivoting_raw_data(df_raw: dd) -> dd:
    df_pivoted = (
        df_raw
        .drop(columns=["Georeferència"])
        .melt(id_vars=["CODI EOI", "NOM ESTACIO", "DATA", "MAGNITUD", "CONTAMINANT", "UNITATS", "TIPUS ESTACIO", "AREA URBANA", 
        "CODI INE", "MUNICIPI", "CODI COMARCA", "NOM COMARCA", "ALTITUD", "LATITUD", "LONGITUD"],
        value_vars=["01h", "02h", "03h", "04h", "05h", "06h", "07h", "08h", "09h", "10h", "11h", "12h", "13h",
        "14h", "15h", "16h", "17h", "18h", "19h", "20h", "21h", "22h", "23h", "24h"],
        var_name="HORA", value_name="VALOR")
    )
    return df_pivoted

dd_pivoted = dd_raw.map_partitions(pivoting_raw_data)
del dd_raw
```

```{python}
#| eval: false

# Convert the date and time columns to a single timestamp column.
def transform_datetime(df_old: dd) -> dd:
    df_old = df_old.assign(
        HORA_tmp = df_old.loc[:,"HORA"].str[:-1].replace("24","00").astype(int)
    )
    df_new = df_old.assign(
        DATA_HORA = dd.to_datetime(
        df_old.loc[:,"DATA"].astype(str) + " " + df_old.loc[:,"HORA_tmp"].astype(str), 
        format="%d/%m/%Y %H"
        )
    ).drop(columns=["DATA", "HORA", "HORA_tmp"])
    return df_new

dd_bronze = dd_pivoted.map_partitions(transform_datetime, meta=transform_datetime(dd_pivoted.head(1)))
del dd_pivoted
```

## Missing data and incorrect values

At this point it's important to check data consistency and missing values, and proceed accordingly depending on the situation.

```{python}
# Plot missing values matrix to identify broad patterns
msno.matrix(dd_bronze.compute()); # global view
msno.matrix(dd_bronze.head(300000)); # local view
```


Only two different types of missing data is shown in the sample: one (more frequent) where the actual measurement ("VALOR") is missing, and another one where the measurement unit and most of the station info is missing.

For the first one, knowing that only ~6% of the info is missing and that it's the most important part for the anaylisis, we will drop these measurements from the dataset. This can be done as there is no apparent pattern in the lack of information (e.g. all the info missing in a defined period).

```{python}
# Plot missing values when removing station lack of info samples
msno.matrix(dd_bronze.dropna(subset="NOM ESTACIO").compute());
```

For the second one, if there is a measurement value in "VALOR" we will try to fill the missing info based on the other rows of the dataset (the stations don't change over time, nor the "MAGNITUD" units).

```{python}
#| eval: false
# Apply missing data plan
def input_missing_data(df_missing: dd) -> dd:
    df_missing = df_missing.dropna(subset=["VALOR"]).reset_index(drop=False) # drop missing measurements
    df_missing = df_missing.replace({"ALTITUD": {0: np.nan}, "LATITUD": {0: np.nan}, "LONGITUD": {0: np.nan}}) # clear wrong geopositional data (0 -> NaN)

    df_missing_info = df_missing.query("`NOM ESTACIO`.isna()").loc[:, ["index", "CODI EOI", "MAGNITUD", "DATA_HORA"]]

    df_missing_eoi_list =  df_missing.query("`NOM ESTACIO`.isna()").loc[:,"CODI EOI"].unique()
    df_correct_info = df_missing.dropna(
        subset=["NOM ESTACIO"]
        ).query(
            "`CODI EOI` in @df_missing_eoi_list",
            local_dict={"df_missing_eoi_list":df_missing_eoi_list}
            ).drop(
                columns=["index", "VALOR", "CODI INE", "CODI COMARCA", "NOM COMARCA", "DATA_HORA"]
            ).groupby(["CODI EOI", "MAGNITUD"]).head(1) # create sample dataframe with correct data to input

    df_merged = dd.merge(df_missing_info, df_correct_info, on=["CODI EOI", "MAGNITUD"], how="left", suffixes=["_x", ""]).drop(columns=["MAGNITUD", "DATA_HORA"]).set_index("index")
    
    df_clean = df_missing.set_index("index").combine_first(df_merged.compute()).reset_index(drop=True) # merge both keeping the correct info if available

    return df_clean.loc[:,['CODI EOI', 'NOM ESTACIO', 'CODI INE', 'MUNICIPI', 'CODI COMARCA', 'NOM COMARCA', 'TIPUS ESTACIO', 'AREA URBANA', 'LATITUD', 'LONGITUD', 'ALTITUD',   
        'MAGNITUD', 'CONTAMINANT', 'UNITATS', 'DATA_HORA', 'VALOR']]

dd_bronze_clean = dd_bronze.map_partitions(input_missing_data, meta=input_missing_data(dd_bronze.head(1)))
catalog.save("dd_log_bronze@dask", dd_bronze_clean)
del dd_bronze, dd_bronze_clean
```

And finally, let's convert the pandas DataFrame to a geopandas GeoDataFrame (with the geometry correctly encoded).

```{python}
#| eval: false

# Create a geodataframe with the geometry taken from the coordinates in "LATITUD" and "LONGITUD"
dd_bronze_clean = catalog.load("dd_log_bronze@pandas")     # from now on we work with pandas and geopandas
def create_geodataframe(df_new: pd.DataFrame) -> gpd.GeoDataFrame:
    temp_geometry = gpd.points_from_xy(
        x=df_new.loc[:,"LONGITUD"],
        y=df_new.loc[:,"LATITUD"],
    )
    temp_gpd = gpd.GeoDataFrame(
        data=df_new.drop(columns=["LATITUD", "LONGITUD"]),
        geometry=temp_geometry,
        crs="EPSG:4326"
    )
    # dgdf_new = dask_geopandas.from_geopandas(temp_gpd)
    return temp_gpd

gdf_bronze = create_geodataframe(dd_bronze_clean)

del dd_bronze_clean
catalog.save("gdf_log_bronze", gdf_bronze)
gdf_bronze.head()
del gdf_bronze
```

## Position of the capture points

Let's have a quick look at all the stations in the first data partition.

```{python}
# Plot all the stations
gdf_bronze = catalog.load("gdf_log_bronze")
alt.data_transformers.enable("vegafusion")
gdf_positions = gdf_bronze.drop_duplicates(subset=["CODI EOI"], keep="first")
gdf_positions.explore(tiles="cartodb positron")
```

Let's plot the different capture stations in the Barcelonès "comarca" and overlap them to the actual map to check their situation.

```{python}
# Plot Barcelonès capture points on the map
gdf_barcelones_bronze = (
    gdf_positions.loc[gdf_positions.loc[:,"NOM COMARCA"]=="Barcelonès",:]
    )
m = folium.Map(
    location=[
        gdf_barcelones_bronze.loc[:,"geometry"].y.mean(),
        gdf_barcelones_bronze.loc[:,"geometry"].x.mean()
    ],
    tiles="cartodb positron",
    zoom_start=12
    )
for i in range(gdf_barcelones_bronze.shape[0]):
    station = gdf_barcelones_bronze.iloc[i,:]
    folium.Marker(
        location=[station.loc["geometry"].y,station.loc["geometry"].x],
        popup=station.loc["NOM ESTACIO"]
        ).add_to(m)     
m
```


Actual data matches the "AREA URBANA" column, showing that only 1 out of 12 points in Barcelonès is located outside an urban area.

```{python}
# Plot "AREA URBANA" for Barcelonès "comarca"
gdf_barcelones_bronze.loc[:,"AREA URBANA"].value_counts()
```

If we pick another "comarca", like Baix Llobregat, and repeat the same process:
```{python}
# Plot Baix Llobregat capture points on the map
gdf_bll_bronze = (
    gdf_positions.loc[gdf_positions.loc[:,"NOM COMARCA"]=="Baix Llobregat",:]
    )
m2 = folium.Map(
    location=[
        gdf_bll_bronze.loc[:,"geometry"].y.mean(),
        gdf_bll_bronze.loc[:,"geometry"].x.mean()
    ],
    tiles="cartodb positron",
    zoom_start=11
    )
for i in range(gdf_bll_bronze.shape[0]):
    station = gdf_bll_bronze.iloc[i,:]
    folium.Marker(
        location=[station.loc["geometry"].y,station.loc["geometry"].x],
        popup=station.loc["NOM ESTACIO"]
        ).add_to(m2)     
m2
```

```{python}
# Plot "AREA URBANA" for Baix Llobregat "comarca"
gdf_bll_bronze.loc[:,"AREA URBANA"].value_counts()
```

We can see that, in this case, the column "AREA URBANA" seems to follow a different criteria than expected, as some points are inside "urban" areas but not categorized as that in the data. We will have to create some features afterwards to categorize better the capture points.

## Peek of the data

Let's select a close-by station and plot the available air quality parameters to see how they look like.

```{python}
# Select a station and show available air quality magnitudes
station = "Barcelona (Eixample)"

closeby_station = gdf_bronze.query("`NOM ESTACIO` == @station").loc[:,["DATA_HORA","VALOR", "CONTAMINANT", "UNITATS"]].sort_values(by="DATA_HORA")
contaminants = closeby_station.loc[:,"CONTAMINANT"].unique()

print(f"Chosen station: {station}")
print(f"Available magnitudes: {contaminants}")
```

```{python}
# Plot the data of the latest 200 measurements for all the available parameters

chart = alt.vconcat().properties(
    title=f"Measured contaminants in {station}"
    )
for i_index in range(0,len(contaminants),2):
    try:
        pair_conts = contaminants[i_index:i_index+2]
    except:
        pair_conts = contaminants[i_index]
    row = alt.hconcat()
    for single_contaminant in pair_conts:
        filtered_closeby = closeby_station.query("CONTAMINANT == @single_contaminant")
        unit = filtered_closeby.loc[:,"UNITATS"].iloc[0]
        row |= alt.Chart(data=filtered_closeby.tail(200)).mark_line().encode(
            x=alt.X("DATA_HORA:T", title="Date"),
            y=alt.Y("VALOR:Q", title=f"{single_contaminant} [{unit}]"),
            tooltip=["DATA_HORA", "VALOR"]
        )
    chart &= row
chart
```

It seems that there are only 7 parameters available for this location, some that share the same unit but ranging in a very different spectrum. It's important to note that different magnitud orders (e.g. "ug/m3" and "mg/m3") could cause some trouble for the learning models, giving much more weight to the bigger magnitude in the prediction: this will have to be taken care in the future.

If we focus on one parameter and have a wider look at it, we can see that there is too much datapoints to have a clear image of any pattern / trend, so we must reduce the frequency of data (by aggregation).

```{python}
# Plot the data of a random parameter with a wider scope
single_contaminant = random.sample(list(contaminants), 1)[0]
print(f"Random air parameter = {single_contaminant}")
filtered_closeby = closeby_station.query("CONTAMINANT == @single_contaminant")
unit = filtered_closeby.loc[:,"UNITATS"].iloc[0]

base = alt.Chart(data=filtered_closeby.tail(10000)).mark_line()
row1 = base.encode(
    x=alt.X("DATA_HORA:T", title="Date"),
    y=alt.Y("VALOR:Q", title=f"{single_contaminant} [{unit}]"),
).properties(title="Raw data (unfiltered)")

row2 = base.encode(
    x=alt.X("week(DATA_HORA):T", title="Date (weeks)"),
    y=alt.Y("mean(VALOR):Q", title=f"{single_contaminant} [{unit}]")
).properties(title="Aggregated data (weekly)")

alt.hconcat(
    row1,
    row2
).resolve_scale(y="shared")
```

```{python}
# ts_closeby = filtered_closeby.filter(["DATA_HORA", "VALOR"]).set_index(pd.to_datetime(filtered_closeby.loc[:,"DATA_HORA"]))
# ts_closeby_resampled7D = ts_closeby.resample("7D").agg(
#     average=("VALOR", "mean"),
#     minimum=("VALOR", "min"),
#     maximum=("VALOR", "max"),
#     stdev=("VALOR", "std")
# )

# base = alt.Chart(data=ts_closeby).encode(
#     x=alt.X("week(DATA_HORA):T")
# )
# chart_q1 = base.mark_line(color="blue", opacity=0.25).encode(
#     y=alt.Y("q1(VALOR):Q")
# )
# char_mean = base.mark_line().encode(
#     y=alt.Y("mean(VALOR):Q"),
#     tooltip="mean(VALOR)"
# )
# chart_q3 = base.mark_line(color="red", opacity=0.25).encode(
#     y=alt.Y("q3(VALOR):Q")
# )
# chart = chart_q1 + chart_q3 + char_mean
# chart
```

```{python}
for parquet_file in glob.glob("air_quality_bronze_*.parquet", root_dir="../data/02_intermediate"):
    temp_gdf = gpd.read_parquet("../data/02_intermediate/"+parquet_file)
    temp_filtered = temp_gdf.query(
        "`NOM ESTACIO` == @station and CONTAMINANT == @single_contaminant"
        ).filter(["DATA_HORA", "VALOR"])
    temp_filtered
```