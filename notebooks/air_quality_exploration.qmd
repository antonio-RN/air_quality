---
title: "Air quality report - data exploration"
format: 
    html:
        # eval: false
        code-fold: true
        code-overflow: scroll
---

## Introduction

This is an exploratory analysis of air quality reports for Catalonia region, updated as of 14/05/2025. 
Data is downloaded from the [open data portal](https://administraciodigital.gencat.cat/ca/dades/dades-obertes/inici/index.html#googtrans(ca|en)) 
of Catalan Government (Generalitat de Catalunya), and includes hourly air quality metrics as well as 
geospatial information of the capture points.

The objective of this analysis is to find the biggest contributors to air pollution in Catalonia for the time period of the data. In order to get to this, both time evolution and geographic position will be taken into account.

## Project workflow

The analysis is structured around a [kedro](https://docs.kedro.org/en/stable/introduction/index.html) 
basic workflow, written in Python and uses the following tools / packages:

- uv (package dependency manager)
- quarto (report renderer)
- pandas (data wrangling)
- altair (data visualization)
- geopandas (geospatial analysis)

The diagram of the different processes applied to the data is shown below:

- Data ingestion and type checking: reading the raw CSV file, converting to appropriate data types, saving "bronze" data to a parquet file.
- Missing data handling: analyzing missing data and handling it properly (removing or imputing), saving "bronze" data to a parquet file.

**WIP --> to be updated as the analysis progresses.**

```{python}
#| eval: false

# Print basic information about the environment and tools used
py_ver = !python --version
print(f"Python version (virtual environment):{py_ver[0]}\n\n")

!kedro -V
```

## Data structure

The raw data is a unique CSV file with the following columns and data types:

```{python}
# Load kedro catalog from YAML
from kedro.io import KedroDataCatalog
import pandas as pd
import yaml

with open("../conf/base/catalog.yml", "r") as f:
    conf_catalog = yaml.safe_load(f)

catalog = KedroDataCatalog.from_config(conf_catalog)
```

```{python}
# Load raw data from the catalog
df_raw = catalog.load("air_quality_log");
df_raw.info()
```

The contained data can be divided in the following categories:

- Identification: columns 0, 1
- Time: column 2 (and partially columns 12-35)
- Location: columns 6, 7, 8, 9, 10, 11, 36, 37, 38, 39 (empty)
- Measurement info: columns 3, 4, 5, 12-35

Data structure doesn't follow a strict "row per measurement" lemma, 
as every row of the file contains 24 data measurements (hourly average per each day).

Some rework will have to be done for easier anaylisis, pivoting the data in order to have
one row for each time measurement per station and per parameter.

Data types seem apropiate to the content. Once pivoted, however, new data types will have to be added, 
such as timestamp for the combined date + hour information. The geographic information, which is plain text
or floats, will have to be converted for geospatial analysis. This will be done by converting the whole data 
type from a pandas normal DataFrame to a geopandas GeoDataFrame (which will retain all of its previous columns plus a 'geometry' column).

However, as the raw data contains 3.4M+ rows (and we will be expanding it even more), the first step will be to split the big dataframe into smaller chunks. Then each chunk will be processed separately and saved as a parquet file.

```{python}
#| eval: false

# Optional filter to speed up processes
# Remove for complete analysis

df_raw = df_raw.iloc[:150000]
df_raw.info()
```

```{python}
#| eval: false

# Pivot the data and save as parquet
def pivoting_raw_data(df_raw: pd.DataFrame) -> pd.DataFrame:
    df_pivoted = (
        df_raw
        .drop(columns=["Georeferència"])
        .melt(id_vars=["CODI EOI", "NOM ESTACIO", "DATA", "MAGNITUD", "CONTAMINANT", "UNITATS", "TIPUS ESTACIO", "AREA URBANA", 
        "CODI INE", "MUNICIPI", "CODI COMARCA", "NOM COMARCA", "ALTITUD", "LATITUD", "LONGITUD"],
        value_vars=["01h", "02h", "03h", "04h", "05h", "06h", "07h", "08h", "09h", "10h", "11h", "12h", "13h",
        "14h", "15h", "16h", "17h", "18h", "19h", "20h", "21h", "22h", "23h", "24h"],
        var_name="HORA", value_name="VALOR")
    )
    return df_pivoted

print(f"Total number of rows: {df_raw.shape[0]}")
max_rows_per_chunk = 350000

index = 1
for i in range(0, df_raw.shape[0], max_rows_per_chunk):
    chunk = df_raw.iloc[i:i+max_rows_per_chunk,:]
    pivoted_chunk = pivoting_raw_data(chunk)
    pivoted_chunk.to_parquet(f"../data/02_intermediate/air_quality_df_{index}.parquet", engine="pyarrow")
    index += 1

del df_raw  
```

```{python}
#| eval: false

# Load the first chunk of data.
df_bronze = pd.read_parquet("../data/02_intermediate/air_quality_df_1.parquet")
```

```{python}
#| eval: false

# Convert the date and time columns to a single timestamp column.
import os
from datetime import datetime, timedelta

def transform_datetime(df_old: pd.DataFrame) -> pd.DataFrame:

    df_old.loc[:, "HORA"] = df_old.loc[:, "HORA"].str.replace("h", "").astype(int)-1
    df_new = df_old.assign(
        DATA_HORA=lambda s: s.apply(
            lambda row: datetime.strptime(
                row.loc["DATA"] + " " + str(row.loc["HORA"]) + ":00"
            , "%d/%m/%Y %H:%M"
            ) + timedelta(hours=1),
        axis=1
    )
    ).drop(columns=["DATA", "HORA"])
    return df_new

df_bronze = transform_datetime(df_bronze)
os.remove("../data/02_intermediate/air_quality_df_1.parquet")
df_bronze.to_parquet("../data/02_intermediate/air_quality_df_1.parquet")
```

## Missing data and incorrect values

At this point it's importante to check data consistency and missing values, and proceed accordingly depending on the situation.

```{python}
# Plot missing values matrix to identify broad patterns
import missingno as msno
df_bronze = pd.read_parquet("../data/02_intermediate/air_quality_df_1.parquet")
msno.matrix(df_bronze); # global view
msno.matrix(df_bronze.iloc[:300000]); # local view
```


Only two different types of missing data is shown in the sample: one (more frequent) where the actual measurement ("VALOR") is missing, and another one where the measurement and most of the station info is missing.

For the first one, knowing that only 6% of the info is missing and that the value missing it's the most important one for the anaylisis, we will drop these measurements from the dataset. This can be done as there is no apparent pattern in the lack of information (e.g. all the info missing in a defined period).

```{python}
# Plot missing values when removing station lack of info samples
df_bronze_partial = df_bronze.dropna(subset="NOM ESTACIO")
msno.matrix(df_bronze_partial);
```

For the second one, if there is a measurement value in "VALOR" we will try to fill the missing info based on the other rows of the dataset (the stations don't change over time).

```{python}
#| eval: false

# Drop missing measurements
df_bronze.dropna(subset=["VALOR"], inplace=True);

df_bronze_missing_eoi =  df_bronze.query("`NOM ESTACIO`.isna()").loc[:,"CODI EOI"].unique()
df_missing_info = df_bronze.dropna(subset=["NOM ESTACIO"]).query("`CODI EOI` in (@df_bronze_missing_eoi)").groupby("CODI EOI").head(1)
```

**WiP --> finish inputing missing values**
```{python}
#| eval: false

# INPUT MISSING VALUES
```

```{python}
#| eval: false

# Create a geodataframe with the geometry taken from the coordinates in "LATITUD" and "LONGITUD"
import geopandas as gpd

def create_geodataframe(df_new: pd.DataFrame) -> gpd.GeoDataFrame:

    gdf_new = gpd.GeoDataFrame(
        df_new, geometry=gpd.points_from_xy(
            df_new.loc[:,"LONGITUD"], df_new.loc[:,"LATITUD"],crs="EPSG:4326"
        )
    ).drop(columns=["LATITUD", "LONGITUD"])
    return gdf_new

gdf_bronze = create_geodataframe(df_bronze)
gdf_bronze.to_parquet("../data/02_intermediate/air_quality_bronze_1.parquet", engine="pyarrow")
del df_bronze
```

```{python}
# Show first rows of the converted geodataframe
import geopandas as gpd
gdf_bronze = gpd.read_parquet("../data/02_intermediate/air_quality_bronze_1.parquet")
gdf_bronze.head()
```

Now that it seems that the data has the correct format, let's apply this functions to the whole dataset and override the previous bronze .parquet files.

```{python}
#| eval: false

# Apply transformations to all the chunks and overwrite .parquet bronze files

for i in range(2, index):
    df_old = pd.read_parquet(f"../data/02_intermediate/air_quality_df_{i}.parquet")
    df_new = transform_datetime(df_old)
    df_new.dropna(subset=["VALOR"], inplace=True)
    # INPUT MISSING DATA
    gdf_new = create_geodataframe(df_new)
    gdf_new.to_parquet(f"../data/02_intermediate/air_quality_bronze_{i}.parquet", engine="pyarrow")

del gdf_bronze
```


## Position of the capture points

Let's have a quick look at all the stations in the first data partition.

```{python}
# Plot all the stations
import altair as alt
gdf_bronze = gpd.read_parquet("../data/02_intermediate/air_quality_bronze_1.parquet")
gdf_positions = gdf_bronze.drop_duplicates(subset=["CODI EOI"], keep="first")
gdf_positions.explore(tiles="cartodb positron")
```

Let's plot the different capture stations in the Barcelonès "comarca" and overlap them to the actual map to check their situation.

```{python}
# Plot Barcelonès capture points on the map
import folium
gdf_barcelones_bronze = (
    gdf_bronze.loc[gdf_bronze.loc[:,"NOM COMARCA"]=="Barcelonès",:].drop_duplicates(subset="CODI EOI", keep="first")
    )
m = folium.Map(
    location=[
        gdf_barcelones_bronze.loc[:,"geometry"].y.mean(),
        gdf_barcelones_bronze.loc[:,"geometry"].x.mean()
    ],
    tiles="cartodb positron",
    zoom_start=12
    )
for i in range(gdf_barcelones_bronze.shape[0]):
    station = gdf_barcelones_bronze.iloc[i,:]
    folium.Marker(
        location=[station.loc["geometry"].y,station.loc["geometry"].x],
        popup=station.loc["NOM ESTACIO"]
        ).add_to(m)     
m
```


Actual data matches the "AREA URBANA" column, showing that only 1 out of 12 points in Barcelonès is located outside an urban area.

```{python}
# Plot "AREA URBANA" for Barcelonès "comarca"
gdf_barcelones_bronze.loc[:,"AREA URBANA"].value_counts()
```

If we pick another "comarca", like Baix Llobregat, and repeat the same process:
```{python}
# Plot Baix Llobregat capture points on the map
gdf_bll_bronze = (
    gdf_bronze.loc[gdf_bronze.loc[:,"NOM COMARCA"]=="Baix Llobregat",:].drop_duplicates(subset="CODI EOI", keep="first")
    )
m2 = folium.Map(
    location=[
        gdf_bll_bronze.loc[:,"geometry"].y.mean(),
        gdf_bll_bronze.loc[:,"geometry"].x.mean()
    ],
    tiles="cartodb positron",
    zoom_start=11
    )
for i in range(gdf_bll_bronze.shape[0]):
    station = gdf_bll_bronze.iloc[i,:]
    folium.Marker(
        location=[station.loc["geometry"].y,station.loc["geometry"].x],
        popup=station.loc["NOM ESTACIO"]
        ).add_to(m2)     
m2
```

```{python}
# Plot "AREA URBANA" for Baix Llobregat "comarca"
gdf_bll_bronze.loc[:,"AREA URBANA"].value_counts()
```

We can see that, in this case, the column "AREA URBANA" seems to follow a different criteria than expected, as some points are inside "urban" areas but not categorized as that in the data. We will have to create some features afterwards to categorize better the capture points.

## Peek of the data

Let's select a close-by station and plot the available air quality parameters to see how they look like.

```{python}
# Select a station and show available air quality magnitudes
station = "Barcelona (Eixample)"
magnitudes = gdf_bronze.loc[gdf_bronze.loc[:,"NOM ESTACIO"] == station,"CONTAMINANT"].unique()

closeby_station = gdf_bronze.query("`NOM ESTACIO` == @station").loc[:,["DATA_HORA","VALOR", "CONTAMINANT", "UNITATS"]].sort_values(by="DATA_HORA")

print(f"Chosen station: {station}")
print(f"Available magnitudes: {magnitudes}")
```

```{python}
# Plot the data of the latest 200 measurements for all the available parameters

chart = alt.vconcat()
for i_index in range(0,len(magnitudes),2):
    try:
        pair_mags = magnitudes[i_index:i_index+2]
    except:
        pair_mags = magnitudes[i_index]
    row = alt.hconcat()
    for magnitude in pair_mags:
        filtered_closeby = closeby_station.query("CONTAMINANT == @magnitude")
        unit = filtered_closeby.loc[:,"UNITATS"].iloc[0]
        row |= alt.Chart(data=filtered_closeby.tail(200)).mark_line().encode(
            x=alt.X("DATA_HORA:T", title="Date"),
            y=alt.Y("VALOR:Q", title=f"{magnitude} [{unit}]"),
            tooltip=["DATA_HORA", "VALOR"]
        )
    chart &= row
chart
```

It seems that there are only 8 parameters available for this location, some that share the same unit but ranging in a very different range.

If we focus on one parameter and have a wider look at it, we can see that there is too much datapoints to have a clear image of any pattern / trend, so we must reduce the frequency of data.

```{python}
# Plot the data of a random parameter with a wider scope
import random as random
magnitude = random.sample(list(magnitudes), 1)
print(f"Random air parameter = {magnitude}")
alt.Chart(data=closeby_station.query("CONTAMINANT == @magnitude").tail(2000)).mark_line().encode(
    x=alt.X("DATA_HORA:T", title="Date"),
    y=alt.Y("VALOR:Q", title=f"{magnitude}"),
    tooltip=["DATA_HORA", "VALOR"]
)
```
