---
title: "Air quality report - data exploration"
format: 
    html:
        #eval: false
        code-fold: true
        code-overflow: scroll
---

## Introduction

This is an exploratory analysis of air quality reports for Catalonia region, updated as of 15/02/2025. 
Data is downloaded from the [open data portal](https://administraciodigital.gencat.cat/ca/dades/dades-obertes/inici/index.html#googtrans(ca|en)) 
of Catalan Government (Generalitat de Catalunya), and includes hourly air quality metrics as well as 
geospatial information of the capture points.

The objective of this analysis is to find the biggest contributors to air pollution in Catalonia for the time period of the data. In order to get to this, both time evolution and geographic position will be taken into account.

```{python}
py_ver = !python --version
print(f"Python version (virtual environment):{py_ver[0]}\n\n")

!kedro -V
```

## Project workflow

The analysis is structured around a [kedro](https://docs.kedro.org/en/stable/introduction/index.html) 
basic workflow, written in Python and uses the following packages:

- uv (package dependency manager)
- quarto (report renderer)
- pandas (data wrangling)
- altair (data visualization)
- geopandas (geospatial analysis)

WIP --> to be updated as the analysis progresses.

## Data structure

The raw data is a unique CSV file with the following columns and data types:

```{python}
from kedro.io import DataCatalog
import pandas as pd
import yaml

# kedro catalog creation and loading
with open("../conf/base/catalog.yml", "r") as f:
    conf_catalog = yaml.safe_load(f)

catalog = DataCatalog.from_config(conf_catalog);
df_raw = catalog.load("air_quality_log");
```

```{python}
df_raw.info()
```

The contained data can be divided in the following categories:

- Identification: columns 0, 1
- Time: column 2 (and partially columns 12-35)
- Location: columns 6, 7, 8, 9, 10, 11, 36, 37, 38, 39 (empty)
- Measurement info: columns 3, 4, 5, 12-35

Data structure doesn't follow a strict "row per measurement" lemma, 
as every row of the file contains 24 data measurements (hourly average per each day).

Some rework will have to be done for easier anaylisis, pivoting the data in order to have
one row for each time measurement per station and per parameter.

```{python}
print(f"Different air parameters or contaminants: {df_raw.loc[:,"CONTAMINANT"].nunique()}")
df_raw.loc[:,"CONTAMINANT"].unique()
```

Data types seem apropiate to the content. Once pivoted, however, new data types will have to be added, 
such as timestamp for the combined date + hour information. The geographic information, which is plain text
or floats, will have to be converted for geospatial analysis. This will be done by converting the whole data 
type from a pandas normal DataFrame to a geopandas GeoDataFrame (which will retain all of its previous columns plus a 'geometry' column).

```{python}
from datetime import datetime, timedelta
import geopandas as gpd

# Pivot the dataframe to have one row per time measurement per parameter
df_bronze = (
    df_raw
    .drop(columns=["Georefer√®ncia"])
    .head(1000000)       # Limit to 1M rows for demonstration purposes, remove for full analysis
    .melt(id_vars=["CODI EOI", "NOM ESTACIO", "DATA", "MAGNITUD", "CONTAMINANT", "UNITATS", "TIPUS ESTACIO", "AREA URBANA", 
    "CODI INE", "MUNICIPI", "CODI COMARCA", "NOM COMARCA", "ALTITUD", "LATITUD", "LONGITUD"],
    value_vars=["01h", "02h", "03h", "04h", "05h", "06h", "07h", "08h", "09h", "10h", "11h", "12h", "13h",
    "14h", "15h", "16h", "17h", "18h", "19h", "20h", "21h", "22h", "23h", "24h"],
    var_name="HORA", value_name="VALOR")
)
```

```{python}
# Convert the date and time columns to a single timestamp column.
df_bronze.loc[:, "HORA"] = df_bronze.loc[:, "HORA"].str.replace("h", "").astype(int)-1

df_bronze = df_bronze.assign(
    DATA_HORA=lambda s: s.apply(
        lambda row: datetime.strptime(
            row.loc["DATA"] + " " + str(row.loc["HORA"]) + ":00"
        , "%d/%m/%Y %H:%M"
        ) + timedelta(hours=1),
    axis=1
)
).drop(columns=["DATA", "HORA"])

del df_raw

# Create a geodataframe with the geometry taken from the coordinates in "LATITUD" and "LONGITUD"
gdf_bronze = gpd.GeoDataFrame(
    df_bronze, geometry=gpd.points_from_xy(
        df_bronze.loc[:,"LATITUD"], df_bronze.loc[:,"LONGITUD"],crs="EPSG:4326"
    )
).drop(columns=["LATITUD", "LONGITUD"])

del df_bronze

gdf_bronze.head()
```

## Peek of the data

Let's select a random station and air quality magnitude to plot it and see how it looks like.

```{python}
import altair as alt

# Select a random station and air quality magnitude
station = gdf_bronze.loc[:,"NOM ESTACIO"].sample(1).values[0]
magnitude = gdf_bronze.loc[:,"CONTAMINANT"].sample(1).values[0]

random_parameter = gdf_bronze.query("`NOM ESTACIO` == @station & CONTAMINANT == @magnitude").loc[:,["DATA_HORA","VALOR", "UNITATS"]].sort_values(by="DATA_HORA")

unit = random_parameter.loc[:,"UNITATS"].unique()[0]

print(f"Random station: {station}")
print(f"Random magnitude: {magnitude}")
print(f"Random mangitude unit: {unit}")
```

```{python}
# Plot the data of the latest 240 measurements
alt.Chart(random_parameter.tail(240)).mark_line().encode(
    x=alt.X("DATA_HORA:T", title="Date"),
    y=alt.Y("VALOR:Q", title=f"Air Quality Parameter {magnitude} [{unit}]"),
    tooltip=["DATA_HORA", "VALOR"]
)
```

