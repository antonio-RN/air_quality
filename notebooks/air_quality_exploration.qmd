---
title: "Air quality report - data exploration"
format: 
    html:
        # eval: false
        code-fold: true
        code-overflow: scroll
---

## Introduction

This is an exploratory analysis of air quality reports for Catalonia region, updated as of 14/05/2025. 
Data is downloaded from the [open data portal](https://administraciodigital.gencat.cat/ca/dades/dades-obertes/inici/index.html#googtrans(ca|en)) 
of Catalan Government (Generalitat de Catalunya), and includes hourly air quality metrics as well as 
geospatial information of the capture points.

The objective of this analysis is to find the biggest contributors to air pollution in Catalonia for the time period of the data. In order to get to this, both time evolution and geographic position will be taken into account.

## Project workflow

The analysis will be structured around a [kedro](https://docs.kedro.org/en/stable/introduction/index.html) 
basic workflow, written in Python and uses the following tools / packages:

- uv (package dependency manager)
- quarto (report renderer)
- pandas (data wrangling)
- altair (data visualization)
- geopandas (geospatial analysis)

The diagram of the different processes applied to the data is shown below:

- [x] Data ingestion and type checking: reading the raw CSV file, converting to appropriate data types, saving data to a parquet file.
- [x] Missing data handling: analyzing missing data and handling it properly (removing or imputing), saving "bronze" data to a parquet file.
- General exploration: checking data overview to spot inconsistencies and get a general grasp of the data "shape".
- [ ] Feature creation: adding new features or characteristics based on geographic knowledge or specific thematic knowledge (air pollution).
- [ ] Time evolution: checking trends through time in the general dataset.
- [ ] Geographic inequalities: checking differences between geographic points and their relationship to them.
- [ ] Combined analysis: mixing time evolution and geographic inequialities to identify broader patterns.

**WIP --> to be updated as the analysis progresses.**

```{python}
#| eval: false

# Print basic information about the environment and tools used
py_ver = !python --version
print(f"Python version (virtual environment):{py_ver[0]}\n\n")

!kedro info
```

## Data structure

The raw data is a unique CSV file with the following columns and data types:

```{python}
%load_ext kedro.ipython
import pandas as pd
```

```{python}
# Load raw data from the catalog
df_raw = catalog.load("air_quality_log");
df_raw.info()
```

The contained data can be divided in the following categories:

- Identification: columns 0, 1
- Time: column 2 (and partially columns 12-35)
- Location: columns 6, 7, 8, 9, 10, 11, 36, 37, 38, 39 (empty)
- Measurement info: columns 3, 4, 5, 12-35

Data structure doesn't follow a strict "row per measurement" lemma, 
as every row of the file contains 24 data measurements (hourly average per each day).

Some rework will have to be done for easier anaylisis, pivoting the data in order to have
one row for each time measurement per station and per parameter.

Data types seem apropiate to the content. Once pivoted, however, new data types will have to be added, 
such as timestamp for the combined date + hour information. The geographic information, which is plain text
or floats, will have to be converted for geospatial analysis. This will be done by converting the whole data 
type from a pandas normal DataFrame to a geopandas GeoDataFrame (which will retain all of its previous columns plus a 'geometry' column).

However, as the raw data contains 3.4M+ rows (and we will be expanding it even more), the first step will be to split the big dataframe into smaller chunks. Then each chunk will be processed separately and saved as a parquet file.

```{python}
#| eval: false

# Optional filter to speed up processes
# Remove for complete analysis

df_raw = df_raw.iloc[:150000]
df_raw.info()
```

```{python}
#| eval: false

# Pivot the data and save as parquet
def pivoting_raw_data(df_raw: pd.DataFrame) -> pd.DataFrame:
    df_pivoted = (
        df_raw
        .drop(columns=["GeoreferÃ¨ncia"])
        .melt(id_vars=["CODI EOI", "NOM ESTACIO", "DATA", "MAGNITUD", "CONTAMINANT", "UNITATS", "TIPUS ESTACIO", "AREA URBANA", 
        "CODI INE", "MUNICIPI", "CODI COMARCA", "NOM COMARCA", "ALTITUD", "LATITUD", "LONGITUD"],
        value_vars=["01h", "02h", "03h", "04h", "05h", "06h", "07h", "08h", "09h", "10h", "11h", "12h", "13h",
        "14h", "15h", "16h", "17h", "18h", "19h", "20h", "21h", "22h", "23h", "24h"],
        var_name="HORA", value_name="VALOR")
    )
    return df_pivoted

print(f"Total number of rows: {df_raw.shape[0]}")
max_rows_per_chunk = 200000

index = 1
for i in range(0, df_raw.shape[0], max_rows_per_chunk):
    chunk = df_raw.iloc[i:i+max_rows_per_chunk,:]
    pivoted_chunk = pivoting_raw_data(chunk)
    pivoted_chunk.to_parquet(f"../data/02_intermediate/air_quality_df_{index}.parquet", engine="pyarrow")
    index += 1

del df_raw  
```

```{python}
#| eval: false

# Load the first chunk of data.
df_bronze = pd.read_parquet("../data/02_intermediate/air_quality_df_1.parquet")
```

```{python}
#| eval: false

# Convert the date and time columns to a single timestamp column.
import os
from datetime import datetime, timedelta

def transform_datetime(df_old: pd.DataFrame) -> pd.DataFrame:

    df_old.loc[:, "HORA"] = df_old.loc[:, "HORA"].str.replace("h", "").astype(int)-1
    df_new = df_old.assign(
        DATA_HORA=lambda s: s.apply(
            lambda row: datetime.strptime(
                row.loc["DATA"] + " " + str(row.loc["HORA"]) + ":00"
            , "%d/%m/%Y %H:%M"
            ) + timedelta(hours=1),
        axis=1
    )
    ).drop(columns=["DATA", "HORA"])
    return df_new

df_bronze = transform_datetime(df_bronze)
os.remove("../data/02_intermediate/air_quality_df_1.parquet")
df_bronze.to_parquet("../data/02_intermediate/air_quality_df_1.parquet")
```

## Missing data and incorrect values

At this point it's important to check data consistency and missing values, and proceed accordingly depending on the situation.

```{python}
# Plot missing values matrix to identify broad patterns
import missingno as msno
df_bronze = pd.read_parquet("../data/02_intermediate/air_quality_df_1.parquet")
msno.matrix(df_bronze); # global view
msno.matrix(df_bronze.iloc[:300000]); # local view
```


Only two different types of missing data is shown in the sample: one (more frequent) where the actual measurement ("VALOR") is missing, and another one where the measurement unit and most of the station info is missing.

For the first one, knowing that only ~6% of the info is missing and that it's the most important part for the anaylisis, we will drop these measurements from the dataset. This can be done as there is no apparent pattern in the lack of information (e.g. all the info missing in a defined period).

```{python}
# Plot missing values when removing station lack of info samples
msno.matrix(df_bronze.dropna(subset="NOM ESTACIO"));
```

For the second one, if there is a measurement value in "VALOR" we will try to fill the missing info based on the other rows of the dataset (the stations don't change over time, nor the "MAGNITUD" units).

```{python}
# Apply missing data plan
def input_missing_data(df_missing: pd.DataFrame) -> pd.DataFrame:
    df_missing = df_missing.dropna(subset=["VALOR"]).reset_index(drop=False); # drop missing measurements

    df_missing_info = df_missing.query("`NOM ESTACIO`.isna()").loc[:, ["index", "CODI EOI", "MAGNITUD", "DATA_HORA"]]

    df_missing_eoi_list =  df_missing.query("`NOM ESTACIO`.isna()").loc[:,"CODI EOI"].unique()
    df_correct_info = df_missing.dropna(
        subset=["NOM ESTACIO"]
        ).query(
            "`CODI EOI` in (@df_missing_eoi_list)"
            ).drop(
                columns=["index", "VALOR", "CODI INE", "CODI COMARCA", "NOM COMARCA", "DATA_HORA"]
            ).groupby(["CODI EOI", "MAGNITUD"]).head(1) # create sample dataframe with correct data to input

    df_merged = pd.merge(df_missing_info, df_correct_info, on=["CODI EOI", "MAGNITUD"], how="left", suffixes=["_x", ""]).drop(columns=["CODI EOI", "MAGNITUD", "DATA_HORA"]).set_index("index")

    df_clean = df_merged.combine_first(df_missing) # merge both keeping the correct info if available

    # WiP -> missing 2nd cleaning for "CODI EOI" missing
    return df_clean.loc[:,['CODI EOI', 'NOM ESTACIO', 'CODI INE', 'MUNICIPI', 'CODI COMARCA', 'NOM COMARCA', 'TIPUS ESTACIO', 'AREA URBANA', 'LATITUD', 'LONGITUD', 'ALTITUD',   
        'MAGNITUD', 'CONTAMINANT', 'UNITATS', 'DATA_HORA', 'VALOR']]
df_bronze_clean = input_missing_data(df_bronze)
del df_bronze
df_bronze_clean.head()
```

And finally, let's convert the pandas DataFrame to a geopandas GeoDataFrame (with the geometry correctly encoded).

```{python}
#| eval: false
# Create a geodataframe with the geometry taken from the coordinates in "LATITUD" and "LONGITUD"
import geopandas as gpd

def create_geodataframe(df_new: pd.DataFrame) -> gpd.GeoDataFrame:

    gdf_new = gpd.GeoDataFrame(
        df_new, geometry=gpd.points_from_xy(
            df_new.loc[:,"LONGITUD"], df_new.loc[:,"LATITUD"],crs="EPSG:4326"
        )
    ).drop(columns=["LATITUD", "LONGITUD"])
    return gdf_new

gdf_bronze = create_geodataframe(df_bronze_clean)
gdf_bronze.to_parquet("../data/02_intermediate/air_quality_bronze_1.parquet", engine="pyarrow")
del df_bronze_clean
```

```{python}
# Show first rows of the converted geodataframe
import geopandas as gpd
gdf_bronze = gpd.read_parquet("../data/02_intermediate/air_quality_bronze_1.parquet")
gdf_bronze.head()
```

Now that it seems that the data has the correct format, let's apply this functions to the whole dataset and override the previous bronze .parquet files.

```{python}
#| eval: false

# Apply transformations to all the chunks and overwrite .parquet bronze files

for i in range(2, index):
    df_old = pd.read_parquet(f"../data/02_intermediate/air_quality_df_{i}.parquet")
    df_new = transform_datetime(df_old)
    df_new = input_missing_data(df_new)
    gdf_new = create_geodataframe(df_new)
    gdf_new.to_parquet(f"../data/02_intermediate/air_quality_bronze_{i}.parquet", engine="pyarrow")
```


## Position of the capture points

Let's have a quick look at all the stations in the first data partition.

```{python}
# Plot all the stations
import altair as alt
import geopandas as gpd
gdf_bronze = gpd.read_parquet("../data/02_intermediate/air_quality_bronze_1.parquet")
gdf_positions = gdf_bronze.drop_duplicates(subset=["CODI EOI"], keep="first")
gdf_positions.explore(tiles="cartodb positron")
```

Let's plot the different capture stations in the BarcelonÃ¨s "comarca" and overlap them to the actual map to check their situation.

```{python}
# Plot BarcelonÃ¨s capture points on the map
import folium
gdf_barcelones_bronze = (
    gdf_bronze.loc[gdf_bronze.loc[:,"NOM COMARCA"]=="BarcelonÃ¨s",:].drop_duplicates(subset="CODI EOI", keep="first")
    )
m = folium.Map(
    location=[
        gdf_barcelones_bronze.loc[:,"geometry"].y.mean(),
        gdf_barcelones_bronze.loc[:,"geometry"].x.mean()
    ],
    tiles="cartodb positron",
    zoom_start=12
    )
for i in range(gdf_barcelones_bronze.shape[0]):
    station = gdf_barcelones_bronze.iloc[i,:]
    folium.Marker(
        location=[station.loc["geometry"].y,station.loc["geometry"].x],
        popup=station.loc["NOM ESTACIO"]
        ).add_to(m)     
m
```


Actual data matches the "AREA URBANA" column, showing that only 1 out of 12 points in BarcelonÃ¨s is located outside an urban area.

```{python}
# Plot "AREA URBANA" for BarcelonÃ¨s "comarca"
gdf_barcelones_bronze.loc[:,"AREA URBANA"].value_counts()
```

If we pick another "comarca", like Baix Llobregat, and repeat the same process:
```{python}
# Plot Baix Llobregat capture points on the map
gdf_bll_bronze = (
    gdf_bronze.loc[gdf_bronze.loc[:,"NOM COMARCA"]=="Baix Llobregat",:].drop_duplicates(subset="CODI EOI", keep="first")
    )
m2 = folium.Map(
    location=[
        gdf_bll_bronze.loc[:,"geometry"].y.mean(),
        gdf_bll_bronze.loc[:,"geometry"].x.mean()
    ],
    tiles="cartodb positron",
    zoom_start=11
    )
for i in range(gdf_bll_bronze.shape[0]):
    station = gdf_bll_bronze.iloc[i,:]
    folium.Marker(
        location=[station.loc["geometry"].y,station.loc["geometry"].x],
        popup=station.loc["NOM ESTACIO"]
        ).add_to(m2)     
m2
```

```{python}
# Plot "AREA URBANA" for Baix Llobregat "comarca"
gdf_bll_bronze.loc[:,"AREA URBANA"].value_counts()
```

We can see that, in this case, the column "AREA URBANA" seems to follow a different criteria than expected, as some points are inside "urban" areas but not categorized as that in the data. We will have to create some features afterwards to categorize better the capture points.

## Peek of the data

Let's select a close-by station and plot the available air quality parameters to see how they look like.

```{python}
# Select a station and show available air quality magnitudes
station = "Barcelona (Eixample)"
magnitudes = gdf_bronze.loc[gdf_bronze.loc[:,"NOM ESTACIO"] == station,"CONTAMINANT"].unique()

closeby_station = gdf_bronze.query("`NOM ESTACIO` == @station").loc[:,["DATA_HORA","VALOR", "CONTAMINANT", "UNITATS"]].sort_values(by="DATA_HORA")

print(f"Chosen station: {station}")
print(f"Available magnitudes: {magnitudes}")
```

```{python}
# Plot the data of the latest 200 measurements for all the available parameters

chart = alt.vconcat()
for i_index in range(0,len(magnitudes),2):
    try:
        pair_mags = magnitudes[i_index:i_index+2]
    except:
        pair_mags = magnitudes[i_index]
    row = alt.hconcat()
    for magnitude in pair_mags:
        filtered_closeby = closeby_station.query("CONTAMINANT == @magnitude")
        unit = filtered_closeby.loc[:,"UNITATS"].iloc[0]
        row |= alt.Chart(data=filtered_closeby.tail(200)).mark_line().encode(
            x=alt.X("DATA_HORA:T", title="Date"),
            y=alt.Y("VALOR:Q", title=f"{magnitude} [{unit}]"),
            tooltip=["DATA_HORA", "VALOR"]
        )
    chart &= row
chart
```

It seems that there are only 8 parameters available for this location, some that share the same unit but ranging in a very different range.

If we focus on one parameter and have a wider look at it, we can see that there is too much datapoints to have a clear image of any pattern / trend, so we must reduce the frequency of data.

```{python}
# Plot the data of a random parameter with a wider scope
import random as random
magnitude = random.sample(list(magnitudes), 1)
print(f"Random air parameter = {magnitude}")
alt.Chart(data=closeby_station.query("CONTAMINANT == @magnitude").tail(2000)).mark_line().encode(
    x=alt.X("DATA_HORA:T", title="Date"),
    y=alt.Y("VALOR:Q", title=f"{magnitude}"),
    tooltip=["DATA_HORA", "VALOR"]
)
```
